---
layout: post
title: TAS Survey
date: 2025-01-20 15:20:48
---





# Temporal Action Segmentation An Analysis of Modern Techniques

时序动作分割：分钟长的视频的密集分割

任务定义、共同基准、监督类型和普遍的评估措施

主要探究**帧表示**和**时间建模**

分类法：监督级别

## 1. 导言

作者认为时间分割是语义分割在时间维度的模拟

时间分割主要的任务/作用：正在执行哪些行动、何时开始、进展到什么程度、这些行动如何改变环境，人们下一步将做什么

与传统基于几秒钟的视频分析动作的方式不同的是，现实中许多活动的持续时间会超出秒级别，达到分钟级别

任务通常有较为松散的任务图（任务顺序）

找到动作边界：1. 学习有区别的帧级表示 2. 对动作之间的时间和顺序关系进行建模

> 动作的排序特征提出了一个基本问题 - 如何对时间或顺序关系进行建模以解释动作重复、持续时间和顺序变化？

现有的综述包括：动作识别，时间动作定位，动作预测

在此过程中，它引入了重复和顺序变化分数，这两个指标表征了动作的时间动态。

> 分析表明，大多数现有数据集在动作重复和顺序变化方面受到限制。此外，还区分了几种性能评估和比较设置。为无监督分割方法提供了标准化的评估设置，以及强调长尾分布的基于类别的评估指标。最后，提出了一些有趣的未来领域和问题供社区调查。

<img src="C:\Users\Fitz_Joye\AppData\Roaming\Typora\typora-user-images\image-20250120161149653.png" alt="image-20250120161149653" style="zoom:67%;" />

## 2. 动作分割入门知识

### 2.1 定义

不同的形式化方式给出了不同的看待任务的视角：

将其视为分段任务，从而以时间片段的角度看待任务

将其看作帧分类任务，从而以逐帧分类的角度看待

（从语义分割的掩码生成角度，或许我们可以有新的分割方式作为参考）

#### 2.2.1 相关任务

任务分类依据：数据域、片段语义识别 以及 片段之间时间动态的推理

### 2.2 数据集

> 突出显示当前未使用的密切相关的程序视频数据集。

![image-20250120222930151](C:\Users\Fitz_Joye\AppData\Roaming\Typora\typora-user-images\image-20250120222930151.png)

**GTEA** 第一视角

**50Salads** 根据预定脚本执行

**YouCookII** 视频的描述，而非动作标签，可以考虑通用动作分割？

#### 数据集分析

固定视角可以根据变化判断动作造成的影响，但动态视角提出了更高的挑战

#### 时间动态性

> 重复分数和顺序变化分数

<img src="C:\Users\Fitz_Joye\AppData\Roaming\Typora\typora-user-images\image-20250121142636697.png" alt="image-20250121142636697" style="zoom: 67%;" />

#### 动作的长尾分布

用于描述逐帧分类方法中，每个动作持续总时间长短的不平衡

<img src="C:\Users\Fitz_Joye\AppData\Roaming\Typora\typora-user-images\image-20250121144354602.png" alt="image-20250121144354602" style="zoom:50%;" />

偏斜度

<img src="C:\Users\Fitz_Joye\AppData\Roaming\Typora\typora-user-images\image-20250121144843597.png" alt="image-20250121144843597" style="zoom:50%;" />

### 2.3 监督

完全监督：有完整的逐帧标注

半监督：部分数据有完整标注，剩下数据无标注或弱标注

弱监督：有以下三种形式，逐次监督性减弱：1. 有每个动作的标志时间戳 2. 仅有动作序列 3. 仅有动作集（无序）4. 最近，[61]表明人们可以取消动作级注释，而简单地使用视频级复杂活动标签进行监督。

无监督：提供动作集合，一次处理同一类视频，从中学习每个动作的匹配



### 2.4 评价

有使用匈牙利匹配算法的无监督模型

将一个动作分成许多不连续的子段称为**过度分割**，更适合通过Edit Score和F1来评估，逐帧的方法MoF(Acc)并不适合

F1 分数和Edit Score都对过度分割有更大的惩罚

![image-20250121152001221](C:\Users\Fitz_Joye\AppData\Roaming\Typora\typora-user-images\image-20250121152001221.png)

### 2.4.2 匈牙利匹配

略，我们用到了再说（还挺有趣的



## 3. 核心技术

逐帧表示（在帧级别提取信息特征以捕获空间外观和运动信息）以及时间和顺序建模（结合时间依赖性和顺序上下文以提高 TAS 准确性）

### 3.1 帧表示

I3D：在Kinetics数据集上预训练，使用光流特征，RGB和光流各1024维，合并得到2048维

TSM：未提及，可以看一下提取方法，同为2048维度

### 3.2 时间和序列建模

帧级别的视作时间建模，片段级别的视作序列建模

**RNN** 然而，逐帧 RNN 或 GRU 的记忆容量不够长，无法捕获动作之间的顺序关系。因此，所讨论的方法[11]、[79]通常与第3.2.2节中介绍的顺序建模技术相结合。而且RNN无法并行操作

**TCN** Encoder-Decoder形式 或 Multi-Stage形式

**Transformer** 缺乏归纳偏置，需要更大的数据集，但现有的数据集都太小了；另一个问题在于长序列下attention难以注意到有益的信息

>  建议参考Vision transformers for action recognition: A survey

### 3.2.2 序列建模

略，都是一些传统方法，似乎并不常用

### 3.2.3 过分割

动作应该是局部连续的

**边界细化** 在 MS-TCN 的后期阶段，使用新颖的局部屏障池操作来平滑具有置信度的边界预测

**高斯平滑 **直接平滑或按顺序相似性计算

**边界平滑** 软边界

<img src="C:\Users\Fitz_Joye\AppData\Roaming\Typora\typora-user-images\image-20250121165934661.png" alt="image-20250121165934661" style="zoom:67%;" />

##  4. 监督级别

### 4.1 完全监督

1. 识别每帧的动作，使用后处理优化
2. 识别分割点
3. 还有一些

### 4.1.1 表示学习

TCN，CNN+BiLSTM，ViT等

### 4.1.2 架构

**TCN**：隐式捕捉动作持续，成对转换，长程依赖

后续的工作用TCN + BiLSTM

ED-TCN：使用可变形卷积

在encoder中，通常的方法会将视频下采样至几帧每秒

| 简称     | 主要贡献                                                     |
| -------- | ------------------------------------------------------------ |
| GatedR   | 添加了一个门控的refine过程                                   |
| UVAST    | 在ASFormer的基础上使用自回归结构；移除了逐帧预测；缓解过度分割 |
| TCTr     | 同时使用TCN和TCTr以平衡性能和复杂度                          |
| FAMMSDTN | 多层空洞transformer，捕捉局部和全局时序关系                  |

### 4.1.3 分割精细化

